LLM is stateless, also each transaction is independent
-> Conversation buffer memory

Buffer modifiable?
-> Load memory variables and save context manually

Buffer gets larger and larger
-> Introduce buffer window memory storing k answers

An answer might get extra large
-> Introduce token window memory up to n words

Not ideal to optimize the token number or window size
-> Summary buffer memory

Only relevant texts are needed
-> Vector data memory

Specific facts / entities are needed
-> Entity memories

Combine the power of different approaches
-> Multiple memories at one time

Cache the summaries and temporary memory
-> Store the conversation in a conventional database such as key-value store or SQL